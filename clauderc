## Quality Checklist (USE THIS EVERY TIME)

Before responding, verify:
- [ ] Did I read and understand the existing code?
- [ ] Did I identify ALL areas that could be affected?
- [ ] Did I provide COMPLETE code (no placeholders)?
- [ ] Did I add error handling EVERYWHERE?
- [ ] Did I validate ALL inputs?
- [ ] Did I include JSDoc comments?
- [ ] Did I follow the existing code style?
- [ ] Did I write unit tests?
- [ ] Did I RUN the unit tests and verify they pass?
- [ ] Did I write API tests (.http or curl)?
- [ ] Did I RUN the API tests and show the results?
- [ ] Did I test for regressions (existing functionality)?
- [ ] Did I RUN the regression tests and show results?
- [ ] Did I provide ACTUAL test output (not just commands)?
- [ ] Did I explain my changes?
- [ ] Did I check for breaking changes?
- [ ] Did I handle edge cases?
- [ ] Would this code pass code review?
- [ ] Can the user copy-paste and run this immediately?
- [ ] Did I prove with test results that everything works?

## Remember
- SLOW DOWN - Speed causes bugs
- THINK FIRST - Then code
- TEST MENTALLY - Walk through execution
- TEST ACTUALLY - Run the tests and show results
- BE COMPLETE - No shortcuts
- BE THOROUGH - Check everything
- BE HELPFUL - Explain clearly
- PROVE IT WORKS - Show test output, not just "it should work"# ============================================
# .clauderc - Claude CLI Configuration (THOROUGH MODE)
# ============================================
# Configured for careful, thorough development with testing

[project]
name = "QE MCP Testing Automation Stack"
description = """
Microservices-based testing automation platform with:
- 27 MCP services (Docker + STDIO)
- .NET code analysis & test generation
- Azure DevOps integration
- Playwright test automation
- React dashboards for visualization
"""

stack = ["Node.js", "Docker", "Express", "React", ".NET Core", "Playwright", "Anthropic API"]

# ============================================
# INSTRUCTIONS - FORCE THOROUGH DEVELOPMENT
# ============================================

[instructions]
content = """
## CRITICAL: Development Process Requirements

### MANDATORY: Before ANY Code Changes
1. **ALWAYS read and understand existing code first**
   - Show me what you found and your understanding
   - Ask clarifying questions if anything is unclear
   - Explain your planned approach before coding

2. **ALWAYS check for related files**
   - Find all files that might be affected
   - List them explicitly before making changes
   - Explain why each file needs to change

3. **ALWAYS provide complete, working code**
   - NO placeholders like "// ... rest of code ..."
   - NO "// Add error handling here" comments
   - Every code block must be complete and executable
   - Include ALL imports, ALL functions, ALL error handling

### MANDATORY: After ANY Code Changes
4. **ALWAYS test your changes mentally first**
   - Walk through the code execution path
   - Identify potential edge cases
   - List what could go wrong
   - Explain how errors are handled

5. **ALWAYS run actual tests before claiming success**
   - Run unit tests: `npm test` or `jest` in the affected directory
   - Run API tests: Execute .http file or curl commands
   - Run integration tests: Test the full request/response flow
   - Run existing test suite to catch regressions
   - Show me the actual test output (pass/fail results)
   - If tests fail, fix the code and re-test
   - DO NOT say "the code should work" - PROVE it works with test results

6. **ALWAYS verify nothing broke**
   - Run the full test suite: `./test-all-mcps.sh` (if available)
   - Test all related endpoints manually
   - Check that all health checks still pass
   - Verify dependent services still function
   - Show me before/after test results

7. **ALWAYS provide verification steps**
   - Give me exact commands to test NEW functionality
   - Give me exact commands to test EXISTING functionality
   - Show expected output vs actual output
   - Provide curl commands or HTTP file examples
   - Include both success and failure test cases
   - Show actual test execution logs

8. **ALWAYS check for breaking changes**
   - Will this break existing functionality?
   - Are there dependencies that need updating?
   - Do any tests need to be modified?
   - List all files that interact with this code
   - Provide migration path if breaking

### MANDATORY: Error Handling Requirements
7. **EVERY function must have error handling**
   - Wrap in try/catch blocks
   - Return structured errors: { error, message, details }
   - Log errors with full context
   - Never let errors silently fail

8. **EVERY API endpoint must validate input**
   - Check for required parameters
   - Validate data types
   - Sanitize user input
   - Return clear error messages

### MANDATORY: Code Quality Standards
9. **Code MUST be production-ready**
   - Add JSDoc comments to ALL functions
   - Use descriptive variable names (no x, y, z)
   - No magic numbers (use named constants)
   - Follow existing code style exactly

10. **Documentation MUST be complete**
    - Every change requires explanation
    - Update README if behavior changes
    - Add inline comments for complex logic
    - Include usage examples

### FORBIDDEN Practices (NEVER DO THESE)
❌ NEVER use placeholders in code
❌ NEVER skip error handling
❌ NEVER omit validation
❌ NEVER forget to test
❌ NEVER make assumptions without asking
❌ NEVER provide partial code
❌ NEVER say "similar pattern" - show the FULL code
❌ NEVER skip explaining your changes
❌ NEVER forget about edge cases
❌ NEVER ignore existing patterns in the codebase

### Required Response Format
When making ANY code change, structure your response as:

**1. ANALYSIS**
   - What I found in the existing code
   - What needs to change and why
   - Potential impacts on other files
   - Edge cases to consider

**2. PLAN**
   - Step-by-step implementation plan
   - Files that will be modified
   - New files that will be created
   - Dependencies that need updating

**3. IMPLEMENTATION**
   - Complete, working code (no placeholders)
   - All necessary imports
   - Full error handling
   - All edge cases handled

**4. VERIFICATION**
   - Exact test commands for NEW functionality
   - ACTUAL TEST RESULTS showing tests passed
   - Exact test commands for EXISTING functionality (regression tests)
   - ACTUAL REGRESSION TEST RESULTS showing nothing broke
   - Expected vs actual output for both
   - Success test cases with real HTTP responses
   - Failure test cases with real error responses
   - Integration test examples with actual output
   - Unit test results: `npm test` output
   - API test results: curl/HTTP file execution results
   - Smoke test results: `./test-all-mcps.sh` output

**5. REGRESSION CHECK**
   - List all existing features that might be affected
   - Test commands to verify each one still works
   - ACTUAL EXECUTION of regression tests
   - Show test output proving nothing broke
   - If anything broke, explain why and provide fix
   - Re-run tests after fix to prove it works

**6. CHECKLIST**
   - [ ] Code is complete (no TODOs)
   - [ ] Error handling added
   - [ ] Input validation added
   - [ ] Tests provided
   - [ ] Documentation updated
   - [ ] No breaking changes OR migration plan provided
   - [ ] Edge cases handled
   - [ ] Follows existing patterns

## Architecture Principles (MUST FOLLOW)

### MCP Structure
- Docker MCPs: MUST have /health endpoint that returns 200
- STDIO MCPs: MUST accept JSON stdin, output JSON stdout
- ALL MCPs: MUST have index.js, package.json, README.md, Dockerfile
- Register in orchestrator/src/services/mcpManager.js
- Add to docker-compose.yml with sequential port

### Coding Standards (NON-NEGOTIABLE)
- ES6 modules: import/export (NOT require/module.exports)
- async/await for ALL async operations (NO callbacks, NO .then())
- File naming: kebab-case (my-service.js)
- Variable naming: camelCase (myVariable)
- Class naming: PascalCase (MyClass)
- Constants: UPPER_SNAKE_CASE (MAX_RETRIES)
- JSDoc comments on EVERY function
- Structured logging: logger.info/error/warn

### Error Handling (REQUIRED EVERYWHERE)
```javascript
// CORRECT - This is the ONLY acceptable pattern
try {
  const result = await someOperation();
  return { success: true, data: result };
} catch (error) {
  logger.error('Operation failed', { error: error.message, stack: error.stack });
  return { 
    success: false, 
    error: 'Operation failed', 
    message: error.message,
    details: process.env.NODE_ENV === 'development' ? error.stack : undefined
  };
}
```

### Input Validation (REQUIRED FOR ALL APIS)
```javascript
// CORRECT - Validate BEFORE processing
app.post('/api/analyze', async (req, res) => {
  // Step 1: Validate required fields
  const { app, type } = req.body;
  if (!app) {
    return res.status(400).json({ 
      error: 'Missing required parameter', 
      message: 'app is required' 
    });
  }

  // Step 2: Validate data types
  if (typeof app !== 'string') {
    return res.status(400).json({ 
      error: 'Invalid parameter type', 
      message: 'app must be a string' 
    });
  }

  // Step 3: Process
  try {
    const result = await processRequest(app, type);
    res.json({ success: true, data: result });
  } catch (error) {
    logger.error('Request failed', { error, app, type });
    res.status(500).json({ 
      error: 'Processing failed', 
      message: error.message 
    });
  }
});
```

### Testing Requirements (MUST PROVIDE AND RUN)
When creating/modifying ANY code:

**Unit Tests (MUST RUN):**
1. Create/update unit tests in tests/ directory
2. Run: `npm test` or `jest` in the service directory
3. Show me the actual test output (all tests passing)
4. If tests fail, fix the code until they pass
5. Aim for 80%+ code coverage

**API Tests (MUST RUN):**
1. Create .http file with test cases for NEW functionality
2. Create regression tests for EXISTING functionality
3. Run each test in the .http file or via curl
4. Show me the actual HTTP responses (status codes, body)
5. Test success cases (200, 201 responses)
6. Test failure cases (400, 404, 500 responses)
7. Verify response structure matches expected schema

**Integration Tests (MUST RUN):**
1. Test the full end-to-end flow
2. Start the service: `docker compose up -d servicename`
3. Wait for health check: `curl http://localhost:PORT/health`
4. Run integration tests
5. Show actual output from running services
6. Test with real dependencies (not mocks)

**Regression Tests (MUST RUN):**
1. Run full test suite: `./test-all-mcps.sh`
2. Test all existing endpoints still work
3. Show before/after comparison
4. If anything breaks, fix it before claiming success

**Mandatory Test Output:**
- Show actual command executed
- Show actual output/response
- Show pass/fail status
- Show any errors or warnings
- Confirm all tests pass before finalizing

**Example Test Evidence Required:**
```bash
# Unit tests
$ cd mcps/risk-analyzer
$ npm test
✓ should analyze risk correctly (45ms)
✓ should handle missing data (12ms)
✓ should validate input (8ms)
Tests: 3 passed, 3 total

# API tests
$ curl http://localhost:8300/health
{"status":"healthy","uptime":123.45}
HTTP/1.1 200 OK ✓

$ curl -X POST http://localhost:8300/analyze -d '{"storyId":123}'
{"success":true,"risk":"medium"}
HTTP/1.1 200 OK ✓

# Regression test
$ ./test-all-mcps.sh
Testing: Risk Analyzer MCP
  ✅ Health check passed
  ✅ API responded successfully
All tests passed!
```

### Docker Conventions (MUST FOLLOW)
- Multi-stage builds for optimization
- Health checks that actually work: `curl -f http://localhost:PORT/health`
- Sequential port numbers (check docker-compose.yml for next available)
- restart: unless-stopped
- Named volumes for persistence
- Proper logging configuration

### Dashboard Development (CSP COMPLIANT)
- NO inline JavaScript (use addEventListener)
- NO onclick attributes in HTML
- Modular functions in script.js
- Progressive enhancement
- Responsive design

## When I Ask You To...

### "Add a feature"
1. First, ask me clarifying questions
2. Show me the existing code structure
3. Identify ALL areas that might be affected
4. List existing functionality that could break
5. Explain your implementation plan
6. Get my approval before coding
7. Provide complete, tested implementation
8. Write unit tests for the new feature
9. RUN unit tests: `npm test` and show output
10. Write API tests (.http file)
11. RUN API tests and show HTTP responses
12. Write regression tests for existing features
13. RUN regression tests and show results
14. Confirm nothing broke with test evidence

### "Fix a bug"
1. Ask me to reproduce the issue
2. Analyze the root cause
3. Identify what else might break from the fix
4. Explain what's wrong
5. Propose a solution
6. Show the fix with tests
7. RUN tests to prove the fix works
8. Provide regression tests to ensure fix doesn't break other things
9. RUN regression tests and show results
10. Explain why it won't happen again

### "Refactor code"
1. Show current code structure
2. Identify ALL files/features that depend on this code
3. Explain what's wrong with it
4. Propose better structure
5. Show complete refactored code
6. Write comprehensive test suite for refactored code
7. RUN all tests before refactoring (baseline)
8. RUN all tests after refactoring (comparison)
9. Show that test results are identical (no regressions)
10. Provide migration steps if needed
11. List all affected files and test each one

### "Add tests"
1. Analyze existing code
2. Identify test cases (success, edge, failure)
3. Write complete test file
4. Show how to run tests
5. Show expected output

## Quality Checklist (USE THIS EVERY TIME)

Before responding, verify:
- [ ] Did I read and understand the existing code?
- [ ] Did I provide COMPLETE code (no placeholders)?
- [ ] Did I add error handling EVERYWHERE?
- [ ] Did I validate ALL inputs?
- [ ] Did I include JSDoc comments?
- [ ] Did I follow the existing code style?
- [ ] Did I provide test commands?
- [ ] Did I explain my changes?
- [ ] Did I check for breaking changes?
- [ ] Did I handle edge cases?
- [ ] Would this code pass code review?
- [ ] Can the user copy-paste and run this immediately?

## Remember
- SLOW DOWN - Speed causes bugs
- THINK FIRST - Then code
- TEST MENTALLY - Walk through execution
- BE COMPLETE - No shortcuts
- BE THOROUGH - Check everything
- BE HELPFUL - Explain clearly
"""

# ============================================
# MODEL CONFIGURATION - USE BETTER MODELS
# ============================================

[models]
# Use Sonnet 3.5 as default for better quality
# (Haiku is fast but makes more mistakes)
default = "claude-sonnet-3-5-20241022"

# Model selection by complexity
simple = "claude-sonnet-3-5-20241022"      # Use Sonnet even for simple tasks
medium = "claude-sonnet-3-5-20241022"      # Sonnet for medium
complex = "claude-sonnet-4-20250514"       # Sonnet 4 for complex

# Disable auto-select to always use default (Sonnet)
auto_select = false

# ============================================
# OUTPUT PREFERENCES - DETAILED EXPLANATIONS
# ============================================

[output]
language = "javascript"
style = "detailed"  # Changed from "concise" to "detailed"
comments = "verbose"  # Changed from "helpful" to "verbose"

# Explanation detail level
explain = "detailed"  # Changed from "medium" to "detailed"

# Always include examples
include_examples = true

# Show reasoning
show_reasoning = true

# Format code properly
format = "prettier"

# ============================================
# CONTEXT MANAGEMENT - MORE CONTEXT
# ============================================

[context]
# Increase files in context
max_files = 30  # Increased from 20

# Increase token context
max_tokens = 100000  # Increased from 50000

# Include dependencies to understand relationships
include_dependencies = true  # Changed from false

# Smart context
smart_context = true

# Boost recently edited files
recent_files_boost = 2.0  # Increased from 1.5

# ============================================
# SAFETY & PROTECTION - BE EXTRA CAREFUL
# ============================================

[safety]
# Protected files (require confirmation)
protected = [
    "package-lock.json",
    "docker-compose.yml",
    ".env",
    ".env.production",
    "data/**",
    "orchestrator/src/services/mcpManager.js",
    "*.config.js",
    ".github/**"
]

# Warn before ANY changes
warn_lines_changed = 50  # Decreased from 100 (warn sooner)
warn_files_modified = 3  # Decreased from 5 (warn sooner)

# Always confirm destructive operations
confirm_delete = true
confirm_overwrite = true
confirm_breaking_change = true

# ============================================
# VERIFICATION REQUIREMENTS
# ============================================

[verification]
# Require these checks before considering code complete
require_tests = true
require_error_handling = true
require_input_validation = true
require_documentation = true
require_examples = true

# Enforce code quality
enforce_jsdoc = true
enforce_error_handling = true
enforce_input_validation = true

# ============================================
# CUSTOM COMMANDS - WITH VERIFICATION
# ============================================

[commands]

# Create new MCP with FULL implementation
add-mcp = """
Create a new MCP service named {0} with COMPLETE, production-ready code.

MANDATORY STEPS (DO NOT SKIP ANY):

1. ANALYSIS PHASE:
   - Check existing MCP structure
   - Find next available port number
   - Identify similar MCPs to follow as pattern
   - List all files that need to be created

2. IMPLEMENTATION PHASE:
   Create these files with COMPLETE code (no placeholders):

   a) mcps/{0}/index.js
      - Express server setup
      - Health check endpoint
      - All API endpoints
      - Full error handling
      - Input validation
      - Logging

   b) mcps/{0}/package.json
      - All dependencies listed
      - Correct scripts
      - Proper metadata

   c) mcps/{0}/Dockerfile
      - Multi-stage build
      - Health check
      - Non-root user
      - Optimized layers

   d) mcps/{0}/README.md
      - Complete documentation
      - API endpoints
      - Examples
      - Setup instructions

   e) mcps/{0}/src/analyzer.js (or generator.js)
      - Complete business logic
      - Error handling
      - JSDoc comments

   f) {0}.http
      - All endpoint tests
      - Success cases
      - Failure cases

3. INTEGRATION PHASE:
   Update these files:
   - orchestrator/src/services/mcpManager.js (add service)
   - docker-compose.yml (add service definition)

4. VERIFICATION PHASE:
   Provide:
   - Docker build command
   - Docker run command
   - Test commands
   - Expected output
   - Troubleshooting steps

5. CHECKLIST:
   - [ ] All files created with complete code
   - [ ] No placeholders or TODOs
   - [ ] Error handling in all functions
   - [ ] Input validation on all endpoints
   - [ ] Health check endpoint works
   - [ ] Service registered in orchestrator
   - [ ] Added to docker-compose.yml
   - [ ] Test file created
   - [ ] README is complete
   - [ ] Can build and run immediately

Show me EVERYTHING. No shortcuts.
"""

# Review code with detailed analysis
review-code = """
Perform THOROUGH code review of {0}:

1. CORRECTNESS:
   - Does the code do what it's supposed to?
   - Are there logical errors?
   - Are edge cases handled?
   - Is error handling complete?

2. COMPLETENESS:
   - Are there any placeholders?
   - Are there any TODOs?
   - Is validation missing?
   - Are imports complete?

3. QUALITY:
   - Code smells?
   - Duplicate code?
   - Magic numbers?
   - Poor naming?

4. SAFETY:
   - Security vulnerabilities?
   - Input validation missing?
   - SQL injection risks?
   - XSS vulnerabilities?

5. TESTING:
   - Can this be tested?
   - Are tests provided?
   - Edge cases covered?

6. DOCUMENTATION:
   - JSDoc comments present?
   - README accurate?
   - Examples provided?

For EACH issue found:
- Severity: Critical/High/Medium/Low
- Location: File and line number
- Problem: What's wrong
- Impact: What could happen
- Solution: How to fix it
- Example: Show the fix

Provide detailed report with code examples for every issue.
"""

# ============================================
# BUDGET - INVEST IN QUALITY
# ============================================

[budget]
# Increased budget for better quality
daily_tokens = 333334  # $5/day for Sonnet 3.5
warn_at = 80
cost_per_task = 0.50  # Higher threshold (quality over speed)
monthly_budget = 100  # $100/month for better results

# ============================================
# LOGGING - TRACK EVERYTHING
# ============================================

[logging]
enabled = true
level = "debug"  # Changed from "info" to "debug"
file = ".claude.log"
max_size = "50mb"  # Increased from 10mb
max_files = 10  # Increased from 5

track_tokens = true
track_costs = true
track_response_time = true
track_errors = true
track_retries = true

# ============================================
# RESPONSE REQUIREMENTS
# ============================================

[response]
# Enforce structured responses
require_analysis = true
require_plan = true
require_tests = true
require_verification = true
require_checklist = true

# Minimum response quality
min_explanation_length = 200  # At least 200 chars of explanation
require_code_examples = true
require_error_examples = true